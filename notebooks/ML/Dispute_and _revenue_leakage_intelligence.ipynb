{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bade8a9",
   "metadata": {},
   "source": [
    "# ============================================\n",
    "# DISPUTE & REVENUE LEAKAGE INTELLIGENCE ANALYSIS\n",
    "# NYC Yellow Taxi Data (2022-2025)\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "086bd435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully\n",
      "Analysis Date: 2025-11-02 15:04:32\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# SECTION 0: IMPORTS & CONFIGURATION\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from glob import glob\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Color scheme for visualizations\n",
    "COLORS = {\n",
    "    'normal': '#2ecc71',\n",
    "    'dispute': '#e74c3c',\n",
    "    'no_charge': '#f39c12',\n",
    "    'voided': '#9b59b6',\n",
    "    'anomaly': '#e67e22',\n",
    "    'primary': '#3498db'\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "588b9b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÅ LOADING DATA...\n",
      "================================================================================\n",
      "Found 44 parquet files\n",
      "‚úì Loaded: yellow_tripdata_2024-09.parquet - 3,022,906 records\n",
      "‚úì Loaded: yellow_tripdata_2024-10.parquet - 3,300,536 records\n",
      "‚úì Loaded: yellow_tripdata_2024-11.parquet - 3,144,873 records\n",
      "‚úì Loaded: yellow_tripdata_2024-12.parquet - 3,198,189 records\n",
      "‚úì Loaded: yellow_tripdata_2025-01.parquet - 2,814,421 records\n",
      "‚úì Loaded: yellow_tripdata_2025-02.parquet - 2,655,706 records\n",
      "‚úì Loaded: yellow_tripdata_2025-03.parquet - 3,078,157 records\n",
      "‚úì Loaded: yellow_tripdata_2025-04.parquet - 3,056,792 records\n",
      "‚úì Loaded: yellow_tripdata_2025-05.parquet - 3,190,706 records\n",
      "‚úì Loaded: yellow_tripdata_2025-06.parquet - 2,909,840 records\n",
      "‚úì Loaded: yellow_tripdata_2025-07.parquet - 2,673,163 records\n",
      "‚úì Loaded: yellow_tripdata_2025-08.parquet - 2,510,535 records\n",
      "\n",
      "‚úÖ Total records loaded: 35,555,824\n",
      "   Date range: 2002-12-31 22:17:43 to 2025-09-01 00:00:29\n",
      "   Memory usage: 6.29 GB\n",
      "\n",
      "‚úì Loaded 265 taxi zones\n",
      "\n",
      "üìä Dataset Overview:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35555824 entries, 0 to 35555823\n",
      "Data columns (total 20 columns):\n",
      " #   Column                 Dtype         \n",
      "---  ------                 -----         \n",
      " 0   VendorID               int32         \n",
      " 1   tpep_pickup_datetime   datetime64[us]\n",
      " 2   tpep_dropoff_datetime  datetime64[us]\n",
      " 3   passenger_count        float64       \n",
      " 4   trip_distance          float64       \n",
      " 5   RatecodeID             float64       \n",
      " 6   store_and_fwd_flag     object        \n",
      " 7   PULocationID           int32         \n",
      " 8   DOLocationID           int32         \n",
      " 9   payment_type           int64         \n",
      " 10  fare_amount            float64       \n",
      " 11  extra                  float64       \n",
      " 12  mta_tax                float64       \n",
      " 13  tip_amount             float64       \n",
      " 14  tolls_amount           float64       \n",
      " 15  improvement_surcharge  float64       \n",
      " 16  total_amount           float64       \n",
      " 17  congestion_surcharge   float64       \n",
      " 18  Airport_fee            float64       \n",
      " 19  cbd_congestion_fee     float64       \n",
      "dtypes: datetime64[us](2), float64(13), int32(3), int64(1), object(1)\n",
      "memory usage: 4.9+ GB\n",
      "None\n",
      "\n",
      "üìä Sample Data:\n",
      "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
      "0         1  2024-09-01 00:05:51   2024-09-01 00:45:03             1.00   \n",
      "1         1  2024-09-01 00:59:35   2024-09-01 01:03:43             1.00   \n",
      "2         2  2024-09-01 00:25:00   2024-09-01 00:34:37             2.00   \n",
      "3         2  2024-09-01 00:31:00   2024-09-01 00:46:52             1.00   \n",
      "4         2  2024-09-01 00:11:57   2024-09-01 00:30:41             2.00   \n",
      "\n",
      "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
      "0           9.80        1.00                  N           138            48   \n",
      "1           0.50        1.00                  N           140           141   \n",
      "2           2.29        1.00                  N           238           152   \n",
      "3           5.20        1.00                  N            93           130   \n",
      "4           2.26        1.00                  N            79           231   \n",
      "\n",
      "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
      "0             1        47.80  10.25     0.50       13.30          6.94   \n",
      "1             1         5.10   3.50     0.50        3.00          0.00   \n",
      "2             2        13.50   1.00     0.50        0.00          0.00   \n",
      "3             1        24.70   1.00     0.50        4.55          0.00   \n",
      "4             1        17.00   1.00     0.50        4.40          0.00   \n",
      "\n",
      "   improvement_surcharge  total_amount  congestion_surcharge  Airport_fee  \\\n",
      "0                   1.00         79.79                  2.50         1.75   \n",
      "1                   1.00         13.10                  2.50         0.00   \n",
      "2                   1.00         16.00                  0.00         0.00   \n",
      "3                   1.00         31.75                  0.00         0.00   \n",
      "4                   1.00         26.40                  2.50         0.00   \n",
      "\n",
      "   cbd_congestion_fee  \n",
      "0                 NaN  \n",
      "1                 NaN  \n",
      "2                 NaN  \n",
      "3                 NaN  \n",
      "4                 NaN  \n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# SECTION 1: DATA LOADING & PREPARATION\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nüìÅ LOADING DATA...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define paths\n",
    "PROCESSED_DATA_PATH = '/Users/yash/Documents/Projects/NYC_Yellow_Taxi_Analytics/data/processed/'\n",
    "ZONE_LOOKUP_PATH = '/Users/yash/Documents/Projects/NYC_Yellow_Taxi_Analytics/data/raw/taxi_zone_lookup.csv'\n",
    "\n",
    "# Find all cleaned parquet files\n",
    "all_files = glob(os.path.join(PROCESSED_DATA_PATH, '**/*.parquet'), recursive=True)\n",
    "print(f\"Found {len(all_files)} parquet files\")\n",
    "\n",
    "# Load data (adjust number of files based on memory constraints)\n",
    "# For full analysis, load all files; for testing, use subset\n",
    "df_list = []\n",
    "files_to_load = sorted(all_files)[-12:]  # Last 12 months\n",
    "\n",
    "for file in files_to_load:\n",
    "    try:\n",
    "        df_temp = pd.read_parquet(file)\n",
    "        df_list.append(df_temp)\n",
    "        print(f\"‚úì Loaded: {os.path.basename(file)} - {len(df_temp):,} records\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error loading {os.path.basename(file)}: {str(e)}\")\n",
    "\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Total records loaded: {len(df):,}\")\n",
    "print(f\"   Date range: {df['tpep_pickup_datetime'].min()} to {df['tpep_pickup_datetime'].max()}\")\n",
    "print(f\"   Memory usage: {df.memory_usage(deep=True).sum() / (1024**3):.2f} GB\")\n",
    "\n",
    "# Load zone lookup\n",
    "zones = pd.read_csv(ZONE_LOOKUP_PATH)\n",
    "print(f\"\\n‚úì Loaded {len(zones)} taxi zones\")\n",
    "\n",
    "# Display initial data info\n",
    "print(\"\\nüìä Dataset Overview:\")\n",
    "print(df.info())\n",
    "print(\"\\nüìä Sample Data:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0953529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß DATA CLEANING & FEATURE ENGINEERING\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£ Creating time-based features...\n",
      "   ‚úì Time features created\n",
      "\n",
      "2Ô∏è‚É£ Creating fare-based features...\n",
      "   ‚úì Fare features created\n",
      "\n",
      "3Ô∏è‚É£ Creating location features...\n",
      "   ‚úì Location features created\n",
      "\n",
      "4Ô∏è‚É£ Cleaning data quality issues...\n",
      "   ‚úì Removed 391 invalid records (0.00%)\n",
      "   ‚úì Valid records: 35,555,433\n",
      "\n",
      "5Ô∏è‚É£ Handling missing values...\n",
      "\n",
      "   Missing values found:\n",
      "      cbd_congestion_fee: 12,666,397 (35.62%)\n",
      "      dropoff_borough: 165,469 (0.47%)\n",
      "      dropoff_zone: 105,382 (0.30%)\n",
      "      pickup_zone: 83,978 (0.24%)\n",
      "      pickup_borough: 8,445 (0.02%)\n",
      "   ‚ÑπÔ∏è airport_fee column not found - created with default value 0\n",
      "   ‚úì Missing values handled\n",
      "\n",
      "6Ô∏è‚É£ Creating payment type labels...\n",
      "\n",
      "üìä Payment Type Distribution:\n",
      "   Credit Card: 30,286,040 (85.18%)\n",
      "   Cash: 4,586,266 (12.90%)\n",
      "   Dispute: 519,604 (1.46%)\n",
      "   No Charge: 163,522 (0.46%)\n",
      "   Unknown: 1 (0.00%)\n",
      "\n",
      "7Ô∏è‚É£ Checking store-and-forward flag...\n",
      "   ‚úì Store-and-forward trips: 124,798 (0.351%)\n",
      "\n",
      "‚úÖ Data cleaning and feature engineering complete!\n",
      "   Final dataset: 35,555,433 records\n",
      "   Features: 37 columns\n",
      "\n",
      "üìã Available columns:\n",
      "['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag', 'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge', 'Airport_fee', 'cbd_congestion_fee', 'trip_duration_minutes', 'pickup_hour', 'pickup_day_of_week', 'is_weekend', 'month_year', 'pickup_date', 'is_rush_hour', 'fare_per_mile', 'fare_per_minute', 'speed_mph', 'pickup_borough', 'pickup_zone', 'dropoff_borough', 'dropoff_zone', 'is_airport', 'airport_fee', 'payment_type_label']\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# SECTION 1.1: DATA CLEANING & FEATURE ENGINEERING\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nüîß DATA CLEANING & FEATURE ENGINEERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Store original count\n",
    "original_count = len(df)\n",
    "\n",
    "# 1. CREATE TIME-BASED FEATURES\n",
    "print(\"\\n1Ô∏è‚É£ Creating time-based features...\")\n",
    "\n",
    "df['trip_duration_minutes'] = (df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
    "df['pickup_hour'] = df['tpep_pickup_datetime'].dt.hour\n",
    "df['pickup_day_of_week'] = df['tpep_pickup_datetime'].dt.dayofweek\n",
    "df['is_weekend'] = df['pickup_day_of_week'].isin([5, 6]).astype(int)\n",
    "df['month_year'] = df['tpep_pickup_datetime'].dt.to_period('M')\n",
    "df['pickup_date'] = df['tpep_pickup_datetime'].dt.date\n",
    "\n",
    "# Define rush hour (7-9 AM or 5-7 PM on weekdays)\n",
    "df['is_rush_hour'] = (\n",
    "    (~df['is_weekend'].astype(bool)) & \n",
    "    (((df['pickup_hour'] >= 7) & (df['pickup_hour'] <= 9)) | \n",
    "     ((df['pickup_hour'] >= 17) & (df['pickup_hour'] <= 19)))\n",
    ").astype(int)\n",
    "\n",
    "print(f\"   ‚úì Time features created\")\n",
    "\n",
    "# 2. CREATE FARE-BASED FEATURES\n",
    "print(\"\\n2Ô∏è‚É£ Creating fare-based features...\")\n",
    "\n",
    "# Handle division by zero\n",
    "df['fare_per_mile'] = np.where(\n",
    "    df['trip_distance'] > 0,\n",
    "    df['fare_amount'] / df['trip_distance'],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "df['fare_per_minute'] = np.where(\n",
    "    df['trip_duration_minutes'] > 0,\n",
    "    df['fare_amount'] / df['trip_duration_minutes'],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "# Speed calculation\n",
    "df['speed_mph'] = np.where(\n",
    "    df['trip_duration_minutes'] > 0,\n",
    "    (df['trip_distance'] / df['trip_duration_minutes']) * 60,\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "print(f\"   ‚úì Fare features created\")\n",
    "\n",
    "# 3. CREATE LOCATION FEATURES\n",
    "print(\"\\n3Ô∏è‚É£ Creating location features...\")\n",
    "\n",
    "# Merge with zone data\n",
    "df = df.merge(\n",
    "    zones[['LocationID', 'Borough', 'Zone']],\n",
    "    left_on='PULocationID',\n",
    "    right_on='LocationID',\n",
    "    how='left'\n",
    ").rename(columns={'Borough': 'pickup_borough', 'Zone': 'pickup_zone'})\n",
    "\n",
    "df = df.merge(\n",
    "    zones[['LocationID', 'Borough', 'Zone']],\n",
    "    left_on='DOLocationID',\n",
    "    right_on='LocationID',\n",
    "    how='left',\n",
    "    suffixes=('_pickup', '_dropoff')\n",
    ").rename(columns={'Borough': 'dropoff_borough', 'Zone': 'dropoff_zone'})\n",
    "\n",
    "df = df.drop(columns=['LocationID_pickup', 'LocationID_dropoff'], errors='ignore')\n",
    "\n",
    "# Airport flag\n",
    "airport_zones = ['JFK Airport', 'LaGuardia Airport', 'Newark Airport']\n",
    "df['is_airport'] = (\n",
    "    df['pickup_zone'].isin(airport_zones) | \n",
    "    df['dropoff_zone'].isin(airport_zones)\n",
    ").astype(int)\n",
    "\n",
    "print(f\"   ‚úì Location features created\")\n",
    "\n",
    "# 4. DATA QUALITY CLEANING\n",
    "print(\"\\n4Ô∏è‚É£ Cleaning data quality issues...\")\n",
    "\n",
    "# Remove obvious data quality issues\n",
    "valid_mask = (\n",
    "    (df['trip_distance'] >= 0) &\n",
    "    (df['trip_distance'] <= 200) &  # Reasonable max distance\n",
    "    (df['trip_duration_minutes'] > 0) &\n",
    "    (df['trip_duration_minutes'] <= 300) &  # Max 5 hours\n",
    "    (df['fare_amount'] >= 0) &\n",
    "    (df['fare_amount'] <= 500) &  # Reasonable max fare\n",
    "    (df['total_amount'] >= 0) &\n",
    "    (df['passenger_count'] >= 0) &\n",
    "    (df['passenger_count'] <= 8) &\n",
    "    # Remove trips with zero distance but positive fare (unless payment issues)\n",
    "    ~((df['trip_distance'] == 0) & (df['fare_amount'] > 5) & (~df['payment_type'].isin([3, 4, 6])))\n",
    ")\n",
    "\n",
    "df = df[valid_mask].copy()\n",
    "\n",
    "removed_count = original_count - len(df)\n",
    "print(f\"   ‚úì Removed {removed_count:,} invalid records ({removed_count/original_count*100:.2f}%)\")\n",
    "print(f\"   ‚úì Valid records: {len(df):,}\")\n",
    "\n",
    "# 5. HANDLE MISSING VALUES\n",
    "print(\"\\n5Ô∏è‚É£ Handling missing values...\")\n",
    "\n",
    "missing_summary = df.isnull().sum()\n",
    "missing_summary = missing_summary[missing_summary > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing_summary) > 0:\n",
    "    print(\"\\n   Missing values found:\")\n",
    "    for col, count in missing_summary.items():\n",
    "        print(f\"      {col}: {count:,} ({count/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    # Fill missing values appropriately - check if columns exist first\n",
    "    if 'pickup_borough' in df.columns:\n",
    "        df['pickup_borough'] = df['pickup_borough'].fillna('Unknown')\n",
    "    if 'dropoff_borough' in df.columns:\n",
    "        df['dropoff_borough'] = df['dropoff_borough'].fillna('Unknown')\n",
    "    if 'pickup_zone' in df.columns:\n",
    "        df['pickup_zone'] = df['pickup_zone'].fillna('Unknown')\n",
    "    if 'dropoff_zone' in df.columns:\n",
    "        df['dropoff_zone'] = df['dropoff_zone'].fillna('Unknown')\n",
    "    if 'congestion_surcharge' in df.columns:\n",
    "        df['congestion_surcharge'] = df['congestion_surcharge'].fillna(0)\n",
    "    if 'airport_fee' in df.columns:\n",
    "        df['airport_fee'] = df['airport_fee'].fillna(0)\n",
    "    else:\n",
    "        # Create airport_fee column if it doesn't exist\n",
    "        df['airport_fee'] = 0\n",
    "        print(\"   ‚ÑπÔ∏è airport_fee column not found - created with default value 0\")\n",
    "    \n",
    "    # Handle other optional columns\n",
    "    if 'cbd_congestion_fee' in df.columns:\n",
    "        df['cbd_congestion_fee'] = df['cbd_congestion_fee'].fillna(0)\n",
    "    else:\n",
    "        df['cbd_congestion_fee'] = 0\n",
    "        print(\"   ‚ÑπÔ∏è cbd_congestion_fee column not found - created with default value 0\")\n",
    "    \n",
    "    print(\"   ‚úì Missing values handled\")\n",
    "else:\n",
    "    print(\"   ‚úì No missing values detected\")\n",
    "    # Still create columns if they don't exist\n",
    "    if 'airport_fee' not in df.columns:\n",
    "        df['airport_fee'] = 0\n",
    "    if 'cbd_congestion_fee' not in df.columns:\n",
    "        df['cbd_congestion_fee'] = 0\n",
    "\n",
    "# 6. PAYMENT TYPE MAPPING\n",
    "print(\"\\n6Ô∏è‚É£ Creating payment type labels...\")\n",
    "\n",
    "payment_type_map = {\n",
    "    0: 'Flex Fare',\n",
    "    1: 'Credit Card',\n",
    "    2: 'Cash',\n",
    "    3: 'No Charge',\n",
    "    4: 'Dispute',\n",
    "    5: 'Unknown',\n",
    "    6: 'Voided'\n",
    "}\n",
    "\n",
    "df['payment_type_label'] = df['payment_type'].map(payment_type_map).fillna('Other')\n",
    "\n",
    "print(\"\\nüìä Payment Type Distribution:\")\n",
    "payment_dist = df['payment_type_label'].value_counts()\n",
    "for payment_type, count in payment_dist.items():\n",
    "    print(f\"   {payment_type}: {count:,} ({count/len(df)*100:.2f}%)\")\n",
    "\n",
    "# 7. STORE-AND-FORWARD FLAG\n",
    "print(\"\\n7Ô∏è‚É£ Checking store-and-forward flag...\")\n",
    "\n",
    "if 'store_and_fwd_flag' in df.columns:\n",
    "    saf_count = (df['store_and_fwd_flag'] == 'Y').sum()\n",
    "    print(f\"   ‚úì Store-and-forward trips: {saf_count:,} ({saf_count/len(df)*100:.3f}%)\")\n",
    "else:\n",
    "    df['store_and_fwd_flag'] = 'N'\n",
    "    print(\"   ‚ÑπÔ∏è store_and_fwd_flag not found - assuming all trips are normal\")\n",
    "\n",
    "print(\"\\n‚úÖ Data cleaning and feature engineering complete!\")\n",
    "print(f\"   Final dataset: {len(df):,} records\")\n",
    "print(f\"   Features: {len(df.columns)} columns\")\n",
    "\n",
    "# Display available columns for debugging\n",
    "print(\"\\nüìã Available columns:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3b2a403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üö® IDENTIFYING PROBLEM TRANSACTIONS\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£ Creating primary problem flags...\n",
      "   ‚úì Dispute trips: 519,604 (1.461%)\n",
      "   ‚úì No-charge trips: 163,522 (0.460%)\n",
      "   ‚úì Voided trips: 0 (0.000%)\n",
      "   ‚úì Total problematic: 683,126 (1.921%)\n",
      "\n",
      "2Ô∏è‚É£ Detecting fare anomalies using IQR method...\n",
      "   ‚úì Fare per mile range: $0.31 - $14.97\n",
      "   ‚úì Fare anomalies: 1,695,664 (4.77%)\n",
      "\n",
      "3Ô∏è‚É£ Detecting distance anomalies...\n",
      "   ‚úì Distance anomalies: 48,802 (0.14%)\n",
      "\n",
      "4Ô∏è‚É£ Detecting duration anomalies...\n",
      "   ‚úì Duration anomalies: 126,356 (0.36%)\n",
      "\n",
      "5Ô∏è‚É£ Detecting suspicious patterns...\n",
      "   ‚úì Suspicious patterns: 81,263 (0.23%)\n",
      "\n",
      "6Ô∏è‚É£ Creating combined anomaly flag...\n",
      "   ‚úì Trips with any anomaly: 1,725,463 (4.85%)\n",
      "\n",
      "üìä PROBLEM TRANSACTION SUMMARY:\n",
      "================================================================================\n",
      "        Category   Count  Percentage\n",
      "        Disputes  519604        1.46\n",
      "       No Charge  163522        0.46\n",
      "          Voided       0        0.00\n",
      "    Fare Anomaly 1695664        4.77\n",
      "Distance Anomaly   48802        0.14\n",
      "Duration Anomaly  126356        0.36\n",
      "      Suspicious   81263        0.23\n",
      "     Any Anomaly 1725463        4.85\n",
      "\n",
      "‚úÖ Problem transaction identification complete!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# SECTION 2: PROBLEM TRANSACTION IDENTIFICATION\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nüö® IDENTIFYING PROBLEM TRANSACTIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. PRIMARY PROBLEM FLAGS\n",
    "print(\"\\n1Ô∏è‚É£ Creating primary problem flags...\")\n",
    "\n",
    "df['is_dispute'] = (df['payment_type'] == 4).astype(int)\n",
    "df['is_no_charge'] = (df['payment_type'] == 3).astype(int)\n",
    "df['is_voided'] = (df['payment_type'] == 6).astype(int)\n",
    "df['is_problematic'] = (df['is_dispute'] | df['is_no_charge'] | df['is_voided']).astype(int)\n",
    "\n",
    "print(f\"   ‚úì Dispute trips: {df['is_dispute'].sum():,} ({df['is_dispute'].mean()*100:.3f}%)\")\n",
    "print(f\"   ‚úì No-charge trips: {df['is_no_charge'].sum():,} ({df['is_no_charge'].mean()*100:.3f}%)\")\n",
    "print(f\"   ‚úì Voided trips: {df['is_voided'].sum():,} ({df['is_voided'].mean()*100:.3f}%)\")\n",
    "print(f\"   ‚úì Total problematic: {df['is_problematic'].sum():,} ({df['is_problematic'].mean()*100:.3f}%)\")\n",
    "\n",
    "# 2. FARE ANOMALY DETECTION (IQR Method)\n",
    "print(\"\\n2Ô∏è‚É£ Detecting fare anomalies using IQR method...\")\n",
    "\n",
    "# Only consider trips with valid fare_per_mile\n",
    "valid_fare_per_mile = df[df['fare_per_mile'].notna() & (df['fare_per_mile'] > 0)]['fare_per_mile']\n",
    "\n",
    "Q1 = valid_fare_per_mile.quantile(0.25)\n",
    "Q3 = valid_fare_per_mile.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "df['is_fare_anomaly'] = (\n",
    "    (df['fare_per_mile'] < lower_bound) | \n",
    "    (df['fare_per_mile'] > upper_bound)\n",
    ").astype(int)\n",
    "\n",
    "print(f\"   ‚úì Fare per mile range: ${lower_bound:.2f} - ${upper_bound:.2f}\")\n",
    "print(f\"   ‚úì Fare anomalies: {df['is_fare_anomaly'].sum():,} ({df['is_fare_anomaly'].mean()*100:.2f}%)\")\n",
    "\n",
    "# 3. DISTANCE ANOMALY DETECTION\n",
    "print(\"\\n3Ô∏è‚É£ Detecting distance anomalies...\")\n",
    "\n",
    "df['is_distance_anomaly'] = (\n",
    "    ((df['trip_distance'] > 100) | \n",
    "     ((df['trip_distance'] < 0.1) & (df['fare_amount'] > 10)))\n",
    ").astype(int)\n",
    "\n",
    "print(f\"   ‚úì Distance anomalies: {df['is_distance_anomaly'].sum():,} ({df['is_distance_anomaly'].mean()*100:.2f}%)\")\n",
    "\n",
    "# 4. DURATION ANOMALY DETECTION\n",
    "print(\"\\n4Ô∏è‚É£ Detecting duration anomalies...\")\n",
    "\n",
    "df['is_duration_anomaly'] = (\n",
    "    (df['trip_duration_minutes'] < 1) | \n",
    "    (df['trip_duration_minutes'] > 180)\n",
    ").astype(int)\n",
    "\n",
    "print(f\"   ‚úì Duration anomalies: {df['is_duration_anomaly'].sum():,} ({df['is_duration_anomaly'].mean()*100:.2f}%)\")\n",
    "\n",
    "# 5. SUSPICIOUS PATTERN DETECTION\n",
    "print(\"\\n5Ô∏è‚É£ Detecting suspicious patterns...\")\n",
    "\n",
    "df['is_suspicious'] = (\n",
    "    # High distance with very low fare\n",
    "    ((df['trip_distance'] > 20) & (df['fare_amount'] < 20)) |\n",
    "    # Very short distance with high fare\n",
    "    ((df['trip_distance'] < 0.5) & (df['fare_amount'] > 50)) |\n",
    "    # High speed (likely data error or toll road)\n",
    "    (df['speed_mph'] > 80) |\n",
    "    # Extremely low speed (stuck in traffic or data error)\n",
    "    ((df['speed_mph'] < 5) & (df['trip_distance'] > 5)) |\n",
    "    # Zero fare but positive total (fees only)\n",
    "    ((df['fare_amount'] == 0) & (df['total_amount'] > 0) & (~df['payment_type'].isin([3, 4, 6]))) |\n",
    "    # Tip exceeds fare (generous or error)\n",
    "    (df['tip_amount'] > df['fare_amount'] * 2)\n",
    ").astype(int)\n",
    "\n",
    "print(f\"   ‚úì Suspicious patterns: {df['is_suspicious'].sum():,} ({df['is_suspicious'].mean()*100:.2f}%)\")\n",
    "\n",
    "# 6. COMBINED ANOMALY FLAG\n",
    "print(\"\\n6Ô∏è‚É£ Creating combined anomaly flag...\")\n",
    "\n",
    "df['has_any_anomaly'] = (\n",
    "    df['is_fare_anomaly'] | \n",
    "    df['is_distance_anomaly'] | \n",
    "    df['is_duration_anomaly'] | \n",
    "    df['is_suspicious']\n",
    ").astype(int)\n",
    "\n",
    "print(f\"   ‚úì Trips with any anomaly: {df['has_any_anomaly'].sum():,} ({df['has_any_anomaly'].mean()*100:.2f}%)\")\n",
    "\n",
    "# 7. SUMMARY STATISTICS\n",
    "print(\"\\nüìä PROBLEM TRANSACTION SUMMARY:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "problem_summary = pd.DataFrame({\n",
    "    'Category': ['Disputes', 'No Charge', 'Voided', 'Fare Anomaly', \n",
    "                 'Distance Anomaly', 'Duration Anomaly', 'Suspicious', 'Any Anomaly'],\n",
    "    'Count': [\n",
    "        df['is_dispute'].sum(),\n",
    "        df['is_no_charge'].sum(),\n",
    "        df['is_voided'].sum(),\n",
    "        df['is_fare_anomaly'].sum(),\n",
    "        df['is_distance_anomaly'].sum(),\n",
    "        df['is_duration_anomaly'].sum(),\n",
    "        df['is_suspicious'].sum(),\n",
    "        df['has_any_anomaly'].sum()\n",
    "    ],\n",
    "    'Percentage': [\n",
    "        df['is_dispute'].mean() * 100,\n",
    "        df['is_no_charge'].mean() * 100,\n",
    "        df['is_voided'].mean() * 100,\n",
    "        df['is_fare_anomaly'].mean() * 100,\n",
    "        df['is_distance_anomaly'].mean() * 100,\n",
    "        df['is_duration_anomaly'].mean() * 100,\n",
    "        df['is_suspicious'].mean() * 100,\n",
    "        df['has_any_anomaly'].mean() * 100\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(problem_summary.to_string(index=False))\n",
    "\n",
    "print(\"\\n‚úÖ Problem transaction identification complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c7cf60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí∞ REVENUE LEAKAGE QUANTIFICATION\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£ DIRECT REVENUE LOSS CALCULATION\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìä Baseline Metrics:\n",
      "   Total trips: 35,555,433\n",
      "   Total revenue: $1,039,869,575.43\n",
      "   Average fare: $29.25\n",
      "\n",
      "üí∏ DIRECT REVENUE LOSSES:\n",
      "\n",
      "   Disputes:\n",
      "      Trips: 519,604\n",
      "      Revenue lost: $15,147,393.45\n",
      "      % of total revenue: 1.457%\n",
      "      Avg loss per trip: $29.15\n",
      "\n",
      "   No Charge:\n",
      "      Trips: 163,522\n",
      "      Revenue lost: $4,227,031.42\n",
      "      % of total revenue: 0.406%\n",
      "      Avg loss per trip: $25.85\n",
      "\n",
      "   Voided:\n",
      "      Trips: 0\n",
      "      Revenue lost: $0.00\n",
      "      % of total revenue: 0.000%\n",
      "      Avg loss per trip: $0.00\n",
      "\n",
      "   ‚ö†Ô∏è TOTAL DIRECT LOSS: $19,374,424.87 (1.863%)\n",
      "\n",
      "\n",
      "2Ô∏è‚É£ ANOMALY REVENUE LOSS CALCULATION\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üìä Benchmark: Median fare per mile = $7.41\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# SECTION 3: REVENUE LEAKAGE QUANTIFICATION\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nüí∞ REVENUE LEAKAGE QUANTIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. DIRECT REVENUE LOSS\n",
    "print(\"\\n1Ô∏è‚É£ DIRECT REVENUE LOSS CALCULATION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Calculate total revenue baseline\n",
    "total_revenue = df['total_amount'].sum()\n",
    "total_trips = len(df)\n",
    "\n",
    "print(f\"\\nüìä Baseline Metrics:\")\n",
    "print(f\"   Total trips: {total_trips:,}\")\n",
    "print(f\"   Total revenue: ${total_revenue:,.2f}\")\n",
    "print(f\"   Average fare: ${df['total_amount'].mean():.2f}\")\n",
    "\n",
    "# Direct losses by category\n",
    "dispute_loss = df[df['is_dispute'] == 1]['total_amount'].sum()\n",
    "no_charge_loss = df[df['is_no_charge'] == 1]['total_amount'].sum()\n",
    "voided_loss = df[df['is_voided'] == 1]['total_amount'].sum()\n",
    "\n",
    "dispute_count = df['is_dispute'].sum()\n",
    "no_charge_count = df['is_no_charge'].sum()\n",
    "voided_count = df['is_voided'].sum()\n",
    "\n",
    "print(f\"\\nüí∏ DIRECT REVENUE LOSSES:\")\n",
    "print(f\"\\n   Disputes:\")\n",
    "print(f\"      Trips: {dispute_count:,}\")\n",
    "print(f\"      Revenue lost: ${dispute_loss:,.2f}\")\n",
    "print(f\"      % of total revenue: {dispute_loss/total_revenue*100:.3f}%\")\n",
    "print(f\"      Avg loss per trip: ${dispute_loss/max(dispute_count,1):.2f}\")\n",
    "\n",
    "print(f\"\\n   No Charge:\")\n",
    "print(f\"      Trips: {no_charge_count:,}\")\n",
    "print(f\"      Revenue lost: ${no_charge_loss:,.2f}\")\n",
    "print(f\"      % of total revenue: {no_charge_loss/total_revenue*100:.3f}%\")\n",
    "print(f\"      Avg loss per trip: ${no_charge_loss/max(no_charge_count,1):.2f}\")\n",
    "\n",
    "print(f\"\\n   Voided:\")\n",
    "print(f\"      Trips: {voided_count:,}\")\n",
    "print(f\"      Revenue lost: ${voided_loss:,.2f}\")\n",
    "print(f\"      % of total revenue: {voided_loss/total_revenue*100:.3f}%\")\n",
    "print(f\"      Avg loss per trip: ${voided_loss/max(voided_count,1):.2f}\")\n",
    "\n",
    "direct_loss_total = dispute_loss + no_charge_loss + voided_loss\n",
    "\n",
    "print(f\"\\n   ‚ö†Ô∏è TOTAL DIRECT LOSS: ${direct_loss_total:,.2f} ({direct_loss_total/total_revenue*100:.3f}%)\")\n",
    "\n",
    "# 2. ANOMALY REVENUE LOSS\n",
    "print(\"\\n\\n2Ô∏è‚É£ ANOMALY REVENUE LOSS CALCULATION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Calculate expected fare based on median fare_per_mile\n",
    "median_fare_per_mile = df[df['fare_per_mile'].notna() & (df['fare_per_mile'] > 0)]['fare_per_mile'].median()\n",
    "\n",
    "print(f\"\\nüìä Benchmark: Median fare per mile = ${median_fare_per_mile:.2f}\")\n",
    "\n",
    "# For anomalous trips, calculate expected vs actual\n",
    "anomaly_trips = df[df['is_fare_anomaly'] == 1].copy()\n",
    "anomaly_trips['expected_fare'] = anomaly_trips['trip_distance'] * median_fare_per_mile\n",
    "anomaly_trips['fare_difference'] = anomaly_trips['fare_amount'] - anomaly_trips['expected_fare']\n",
    "\n",
    "# Separate into overcharges and undercharges\n",
    "undercharges = anomaly_trips[anomaly_trips['fare_difference'] < 0]\n",
    "overcharges = anomaly_trips[anomaly_trips['fare_difference'] > 0]\n",
    "\n",
    "undercharge_loss = abs(undercharges['fare_difference'].sum())\n",
    "overcharge_gain = overcharges['fare_difference'].sum()\n",
    "net_anomaly_loss = undercharge_loss - overcharge_gain\n",
    "\n",
    "print(f\"\\nüí∏ ANOMALY REVENUE IMPACT:\")\n",
    "print(f\"\\n   Undercharges:\")\n",
    "print(f\"      Trips: {len(undercharges):,}\")\n",
    "print(f\"      Revenue lost: ${undercharge_loss:,.2f}\")\n",
    "print(f\"      Avg undercharge: ${undercharge_loss/max(len(undercharges),1):.2f}\")\n",
    "\n",
    "print(f\"\\n   Overcharges:\")\n",
    "print(f\"      Trips: {len(overcharges):,}\")\n",
    "print(f\"      Revenue gained: ${overcharge_gain:,.2f}\")\n",
    "print(f\"      Avg overcharge: ${overcharge_gain/max(len(overcharges),1):.2f}\")\n",
    "\n",
    "print(f\"\\n   NET ANOMALY LOSS: ${net_anomaly_loss:,.2f}\")\n",
    "\n",
    "# 3. TOTAL REVENUE LEAKAGE\n",
    "print(\"\\n\\n3Ô∏è‚É£ TOTAL REVENUE LEAKAGE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create comprehensive summary table\n",
    "leakage_summary = pd.DataFrame({\n",
    "    'Loss Category': [\n",
    "        'Disputes',\n",
    "        'No Charge',\n",
    "        'Voided',\n",
    "        'Fare Anomalies (Net)',\n",
    "        '‚îÄ' * 30,\n",
    "        'TOTAL LEAKAGE'\n",
    "    ],\n",
    "    'Trip Count': [\n",
    "        dispute_count,\n",
    "        no_charge_count,\n",
    "        voided_count,\n",
    "        len(anomaly_trips),\n",
    "        '‚îÄ' * 10,\n",
    "        dispute_count + no_charge_count + voided_count\n",
    "    ],\n",
    "    'Total $ Lost': [\n",
    "        f'${dispute_loss:,.2f}',\n",
    "        f'${no_charge_loss:,.2f}',\n",
    "        f'${voided_loss:,.2f}',\n",
    "        f'${net_anomaly_loss:,.2f}',\n",
    "        '‚îÄ' * 15,\n",
    "        f'${direct_loss_total + net_anomaly_loss:,.2f}'\n",
    "    ],\n",
    "    '% of Total Revenue': [\n",
    "        f'{dispute_loss/total_revenue*100:.3f}%',\n",
    "        f'{no_charge_loss/total_revenue*100:.3f}%',\n",
    "        f'{voided_loss/total_revenue*100:.3f}%',\n",
    "        f'{net_anomaly_loss/total_revenue*100:.3f}%',\n",
    "        '‚îÄ' * 8,\n",
    "        f'{(direct_loss_total + net_anomaly_loss)/total_revenue*100:.3f}%'\n",
    "    ],\n",
    "    'Avg Loss per Trip': [\n",
    "        f'${dispute_loss/max(dispute_count,1):.2f}',\n",
    "        f'${no_charge_loss/max(no_charge_count,1):.2f}',\n",
    "        f'${voided_loss/max(voided_count,1):.2f}',\n",
    "        f'${net_anomaly_loss/max(len(anomaly_trips),1):.2f}',\n",
    "        '‚îÄ' * 8,\n",
    "        f'${(direct_loss_total + net_anomaly_loss)/max(dispute_count + no_charge_count + voided_count,1):.2f}'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + leakage_summary.to_string(index=False))\n",
    "\n",
    "# 4. ANNUALIZED PROJECTION\n",
    "print(\"\\n\\n4Ô∏è‚É£ ANNUALIZED IMPACT PROJECTION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Calculate date range\n",
    "date_range_days = (df['pickup_date'].max() - df['pickup_date'].min()).days\n",
    "years_covered = date_range_days / 365.25\n",
    "\n",
    "total_leakage = direct_loss_total + net_anomaly_loss\n",
    "\n",
    "if years_covered < 1:\n",
    "    annual_leakage = (total_leakage / date_range_days) * 365.25\n",
    "    print(f\"\\n   Data coverage: {date_range_days} days ({years_covered:.2f} years)\")\n",
    "    print(f\"   Total leakage in period: ${total_leakage:,.2f}\")\n",
    "    print(f\"   üìà PROJECTED ANNUAL LEAKAGE: ${annual_leakage:,.2f}\")\n",
    "else:\n",
    "    annual_leakage = total_leakage / years_covered\n",
    "    print(f\"\\n   Data coverage: {date_range_days} days ({years_covered:.2f} years)\")\n",
    "    print(f\"   Total leakage in period: ${total_leakage:,.2f}\")\n",
    "    print(f\"   üìà AVERAGE ANNUAL LEAKAGE: ${annual_leakage:,.2f}\")\n",
    "\n",
    "# Store key metrics for later use\n",
    "leakage_metrics = {\n",
    "    'total_revenue': total_revenue,\n",
    "    'total_trips': total_trips,\n",
    "    'dispute_loss': dispute_loss,\n",
    "    'no_charge_loss': no_charge_loss,\n",
    "    'voided_loss': voided_loss,\n",
    "    'direct_loss_total': direct_loss_total,\n",
    "    'net_anomaly_loss': net_anomaly_loss,\n",
    "    'total_leakage': total_leakage,\n",
    "    'annual_leakage': annual_leakage,\n",
    "    'leakage_percentage': (total_leakage / total_revenue) * 100\n",
    "}\n",
    "\n",
    "print(\"\\n‚úÖ Revenue leakage quantification complete!\")\n",
    "print(f\"\\nüö® KEY FINDING: ${total_leakage:,.2f} in total revenue leakage\")\n",
    "print(f\"   This represents {leakage_metrics['leakage_percentage']:.3f}% of total revenue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1979485d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SECTION 4: TEMPORAL PATTERN ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n‚è∞ TEMPORAL PATTERN ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. HOURLY PATTERNS\n",
    "print(\"\\n1Ô∏è‚É£ ANALYZING HOURLY PATTERNS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "hourly_analysis = df.groupby('pickup_hour').agg({\n",
    "    'is_dispute': ['sum', 'mean'],\n",
    "    'is_no_charge': ['sum', 'mean'],\n",
    "    'is_voided': ['sum', 'mean'],\n",
    "    'is_problematic': ['sum', 'mean'],\n",
    "    'total_amount': ['sum', 'mean', 'count']\n",
    "}).reset_index()\n",
    "\n",
    "hourly_analysis.columns = ['hour', 'dispute_count', 'dispute_rate', \n",
    "                           'no_charge_count', 'no_charge_rate',\n",
    "                           'voided_count', 'voided_rate',\n",
    "                           'problem_count', 'problem_rate',\n",
    "                           'total_revenue', 'avg_fare', 'trip_count']\n",
    "\n",
    "# Calculate revenue loss per hour\n",
    "hourly_problem_revenue = df[df['is_problematic'] == 1].groupby('pickup_hour')['total_amount'].sum().reset_index()\n",
    "hourly_problem_revenue.columns = ['hour', 'revenue_lost']\n",
    "hourly_analysis = hourly_analysis.merge(hourly_problem_revenue, on='hour', how='left').fillna(0)\n",
    "\n",
    "print(\"\\nüìä Hourly Problem Rate Summary:\")\n",
    "print(hourly_analysis[['hour', 'trip_count', 'problem_rate', 'revenue_lost']].to_string(index=False))\n",
    "\n",
    "# Identify peak problem hours\n",
    "top_problem_hours = hourly_analysis.nlargest(5, 'problem_rate')[['hour', 'problem_rate', 'revenue_lost']]\n",
    "print(f\"\\n‚ö†Ô∏è TOP 5 PROBLEM HOURS:\")\n",
    "print(top_problem_hours.to_string(index=False))\n",
    "\n",
    "# 2. DAY OF WEEK PATTERNS\n",
    "print(\"\\n\\n2Ô∏è‚É£ ANALYZING DAY OF WEEK PATTERNS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "dow_map = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', \n",
    "           4: 'Friday', 5: 'Saturday', 6: 'Sunday'}\n",
    "\n",
    "dow_analysis = df.groupby('pickup_day_of_week').agg({\n",
    "    'is_dispute': ['sum', 'mean'],\n",
    "    'is_no_charge': ['sum', 'mean'],\n",
    "    'is_voided': ['sum', 'mean'],\n",
    "    'is_problematic': ['sum', 'mean'],\n",
    "    'total_amount': ['sum', 'count']\n",
    "}).reset_index()\n",
    "\n",
    "dow_analysis.columns = ['day_of_week', 'dispute_count', 'dispute_rate',\n",
    "                        'no_charge_count', 'no_charge_rate',\n",
    "                        'voided_count', 'voided_rate',\n",
    "                        'problem_count', 'problem_rate',\n",
    "                        'total_revenue', 'trip_count']\n",
    "\n",
    "dow_analysis['day_name'] = dow_analysis['day_of_week'].map(dow_map)\n",
    "\n",
    "# Calculate revenue loss per day\n",
    "dow_problem_revenue = df[df['is_problematic'] == 1].groupby('pickup_day_of_week')['total_amount'].sum().reset_index()\n",
    "dow_problem_revenue.columns = ['day_of_week', 'revenue_lost']\n",
    "dow_analysis = dow_analysis.merge(dow_problem_revenue, on='day_of_week', how='left').fillna(0)\n",
    "\n",
    "print(\"\\nüìä Day of Week Problem Rate Summary:\")\n",
    "print(dow_analysis[['day_name', 'trip_count', 'problem_rate', 'revenue_lost']].to_string(index=False))\n",
    "\n",
    "# Weekend vs Weekday comparison\n",
    "weekend_problem_rate = df[df['is_weekend'] == 1]['is_problematic'].mean()\n",
    "weekday_problem_rate = df[df['is_weekend'] == 0]['is_problematic'].mean()\n",
    "\n",
    "print(f\"\\nüìÖ WEEKEND vs WEEKDAY:\")\n",
    "print(f\"   Weekend problem rate: {weekend_problem_rate*100:.3f}%\")\n",
    "print(f\"   Weekday problem rate: {weekday_problem_rate*100:.3f}%\")\n",
    "print(f\"   Difference: {(weekend_problem_rate - weekday_problem_rate)*100:+.3f} percentage points\")\n",
    "\n",
    "# 3. MONTHLY TRENDS\n",
    "print(\"\\n\\n3Ô∏è‚É£ ANALYZING MONTHLY TRENDS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "monthly_analysis = df.groupby('month_year').agg({\n",
    "    'is_dispute': ['sum', 'mean'],\n",
    "    'is_no_charge': ['sum', 'mean'],\n",
    "    'is_voided': ['sum', 'mean'],\n",
    "    'is_problematic': ['sum', 'mean'],\n",
    "    'total_amount': ['sum', 'count']\n",
    "}).reset_index()\n",
    "\n",
    "monthly_analysis.columns = ['month', 'dispute_count', 'dispute_rate',\n",
    "                            'no_charge_count', 'no_charge_rate',\n",
    "                            'voided_count', 'voided_rate',\n",
    "                            'problem_count', 'problem_rate',\n",
    "                            'total_revenue', 'trip_count']\n",
    "\n",
    "# Calculate revenue loss per month\n",
    "monthly_problem_revenue = df[df['is_problematic'] == 1].groupby('month_year')['total_amount'].sum().reset_index()\n",
    "monthly_problem_revenue.columns = ['month', 'revenue_lost']\n",
    "monthly_analysis = monthly_analysis.merge(monthly_problem_revenue, on='month', how='left').fillna(0)\n",
    "\n",
    "monthly_analysis['month_str'] = monthly_analysis['month'].astype(str)\n",
    "\n",
    "print(\"\\nüìä Monthly Trend Summary:\")\n",
    "print(monthly_analysis[['month_str', 'trip_count', 'problem_rate', 'revenue_lost']].to_string(index=False))\n",
    "\n",
    "# Trend analysis\n",
    "if len(monthly_analysis) > 1:\n",
    "    first_month_rate = monthly_analysis.iloc[0]['problem_rate']\n",
    "    last_month_rate = monthly_analysis.iloc[-1]['problem_rate']\n",
    "    trend_change = last_month_rate - first_month_rate\n",
    "    \n",
    "    print(f\"\\nüìà TREND ANALYSIS:\")\n",
    "    print(f\"   First month problem rate: {first_month_rate*100:.3f}%\")\n",
    "    print(f\"   Last month problem rate: {last_month_rate*100:.3f}%\")\n",
    "    print(f\"   Change: {trend_change*100:+.3f} percentage points\")\n",
    "    \n",
    "    if trend_change > 0.001:\n",
    "        print(f\"   ‚ö†Ô∏è ALERT: Problem rates are INCREASING\")\n",
    "    elif trend_change < -0.001:\n",
    "        print(f\"   ‚úÖ GOOD NEWS: Problem rates are DECREASING\")\n",
    "    else:\n",
    "        print(f\"   ‚Üí Problem rates are STABLE\")\n",
    "\n",
    "# 4. RUSH HOUR ANALYSIS\n",
    "print(\"\\n\\n4Ô∏è‚É£ RUSH HOUR vs OFF-PEAK COMPARISON\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# FIX: Use separate aggregations to avoid MultiIndex confusion\n",
    "rush_hour_df = df[df['is_rush_hour'] == 1]\n",
    "off_peak_df = df[df['is_rush_hour'] == 0]\n",
    "\n",
    "rush_hour_trips = len(rush_hour_df)\n",
    "rush_hour_problem_count = rush_hour_df['is_problematic'].sum()\n",
    "rush_hour_problem_rate = rush_hour_df['is_problematic'].mean()\n",
    "rush_hour_revenue_lost = rush_hour_df[rush_hour_df['is_problematic'] == 1]['total_amount'].sum()\n",
    "\n",
    "off_peak_trips = len(off_peak_df)\n",
    "off_peak_problem_count = off_peak_df['is_problematic'].sum()\n",
    "off_peak_problem_rate = off_peak_df['is_problematic'].mean()\n",
    "off_peak_revenue_lost = off_peak_df[off_peak_df['is_problematic'] == 1]['total_amount'].sum()\n",
    "\n",
    "print(f\"\\nüìä RUSH HOUR (7-9 AM, 5-7 PM Weekdays):\")\n",
    "print(f\"   Trips: {rush_hour_trips:,}\")\n",
    "print(f\"   Problem rate: {rush_hour_problem_rate*100:.3f}%\")\n",
    "print(f\"   Problem trips: {rush_hour_problem_count:,}\")\n",
    "print(f\"   Revenue lost: ${rush_hour_revenue_lost:,.2f}\")\n",
    "\n",
    "print(f\"\\nüìä OFF-PEAK:\")\n",
    "print(f\"   Trips: {off_peak_trips:,}\")\n",
    "print(f\"   Problem rate: {off_peak_problem_rate*100:.3f}%\")\n",
    "print(f\"   Problem trips: {off_peak_problem_count:,}\")\n",
    "print(f\"   Revenue lost: ${off_peak_revenue_lost:,.2f}\")\n",
    "\n",
    "print(f\"\\n   Difference: {(rush_hour_problem_rate - off_peak_problem_rate)*100:+.3f} percentage points\")\n",
    "\n",
    "if rush_hour_problem_rate > off_peak_problem_rate:\n",
    "    print(f\"   ‚ö†Ô∏è ALERT: Rush hour has {((rush_hour_problem_rate / off_peak_problem_rate) - 1) * 100:.1f}% MORE problems\")\n",
    "else:\n",
    "    print(f\"   ‚úÖ Rush hour has {((1 - rush_hour_problem_rate / off_peak_problem_rate)) * 100:.1f}% FEWER problems\")\n",
    "\n",
    "print(\"\\n‚úÖ Temporal pattern analysis complete!\")\n",
    "\n",
    "# Store for visualizations\n",
    "temporal_data = {\n",
    "    'hourly': hourly_analysis,\n",
    "    'dow': dow_analysis,\n",
    "    'monthly': monthly_analysis\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43798306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SECTION 5: GEOGRAPHIC HOTSPOT ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nüó∫Ô∏è GEOGRAPHIC HOTSPOT ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. PICKUP ZONE ANALYSIS\n",
    "print(\"\\n1Ô∏è‚É£ ANALYZING PICKUP ZONES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "pickup_analysis = df.groupby('PULocationID').agg({\n",
    "    'is_dispute': ['sum', 'mean'],\n",
    "    'is_no_charge': ['sum', 'mean'],\n",
    "    'is_voided': ['sum', 'mean'],\n",
    "    'is_problematic': ['sum', 'mean'],\n",
    "    'total_amount': ['sum', 'count'],\n",
    "    'is_fare_anomaly': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "pickup_analysis.columns = ['LocationID', 'dispute_count', 'dispute_rate',\n",
    "                           'no_charge_count', 'no_charge_rate',\n",
    "                           'voided_count', 'voided_rate',\n",
    "                           'problem_count', 'problem_rate',\n",
    "                           'total_revenue', 'trip_count',\n",
    "                           'anomaly_score']\n",
    "\n",
    "# Calculate revenue lost per zone\n",
    "pickup_revenue_lost = df[df['is_problematic'] == 1].groupby('PULocationID')['total_amount'].sum().reset_index()\n",
    "pickup_revenue_lost.columns = ['LocationID', 'revenue_lost']\n",
    "pickup_analysis = pickup_analysis.merge(pickup_revenue_lost, on='LocationID', how='left').fillna(0)\n",
    "\n",
    "# Merge with zone names\n",
    "pickup_analysis = pickup_analysis.merge(\n",
    "    zones[['LocationID', 'Borough', 'Zone']],\n",
    "    on='LocationID',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Calculate problem score (weighted combination of metrics)\n",
    "pickup_analysis['problem_score'] = (\n",
    "    pickup_analysis['problem_rate'] * 0.4 +\n",
    "    pickup_analysis['anomaly_score'] * 0.3 +\n",
    "    (pickup_analysis['revenue_lost'] / pickup_analysis['revenue_lost'].max()) * 0.3\n",
    ")\n",
    "\n",
    "# Sort by revenue lost\n",
    "pickup_analysis = pickup_analysis.sort_values('revenue_lost', ascending=False)\n",
    "\n",
    "print(\"\\nüìä TOP 20 PROBLEM PICKUP ZONES (by Revenue Lost):\")\n",
    "print(pickup_analysis[['Zone', 'Borough', 'trip_count', 'problem_rate', 'revenue_lost']].head(20).to_string(index=False))\n",
    "\n",
    "# 2. DROPOFF ZONE ANALYSIS\n",
    "print(\"\\n\\n2Ô∏è‚É£ ANALYZING DROPOFF ZONES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "dropoff_analysis = df.groupby('DOLocationID').agg({\n",
    "    'is_dispute': ['sum', 'mean'],\n",
    "    'is_no_charge': ['sum', 'mean'],\n",
    "    'is_voided': ['sum', 'mean'],\n",
    "    'is_problematic': ['sum', 'mean'],\n",
    "    'total_amount': ['sum', 'count'],\n",
    "    'is_fare_anomaly': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "dropoff_analysis.columns = ['LocationID', 'dispute_count', 'dispute_rate',\n",
    "                            'no_charge_count', 'no_charge_rate',\n",
    "                            'voided_count', 'voided_rate',\n",
    "                            'problem_count', 'problem_rate',\n",
    "                            'total_revenue', 'trip_count',\n",
    "                            'anomaly_score']\n",
    "\n",
    "# Calculate revenue lost per zone\n",
    "dropoff_revenue_lost = df[df['is_problematic'] == 1].groupby('DOLocationID')['total_amount'].sum().reset_index()\n",
    "dropoff_revenue_lost.columns = ['LocationID', 'revenue_lost']\n",
    "dropoff_analysis = dropoff_analysis.merge(dropoff_revenue_lost, on='LocationID', how='left').fillna(0)\n",
    "\n",
    "# Merge with zone names\n",
    "dropoff_analysis = dropoff_analysis.merge(\n",
    "    zones[['LocationID', 'Borough', 'Zone']],\n",
    "    on='LocationID',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "dropoff_analysis = dropoff_analysis.sort_values('revenue_lost', ascending=False)\n",
    "\n",
    "print(\"\\nüìä TOP 20 PROBLEM DROPOFF ZONES (by Revenue Lost):\")\n",
    "print(dropoff_analysis[['Zone', 'Borough', 'trip_count', 'problem_rate', 'revenue_lost']].head(20).to_string(index=False))\n",
    "\n",
    "# 3. ROUTE-LEVEL ANALYSIS\n",
    "print(\"\\n\\n3Ô∏è‚É£ ANALYZING HIGH-RISK ROUTES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "route_analysis = df.groupby(['PULocationID', 'DOLocationID']).agg({\n",
    "    'is_problematic': ['sum', 'mean'],\n",
    "    'total_amount': ['sum', 'count']\n",
    "}).reset_index()\n",
    "\n",
    "route_analysis.columns = ['PULocationID', 'DOLocationID', \n",
    "                         'problem_count', 'problem_rate',\n",
    "                         'total_revenue', 'trip_count']\n",
    "\n",
    "# Calculate revenue at risk\n",
    "route_revenue_lost = df[df['is_problematic'] == 1].groupby(['PULocationID', 'DOLocationID'])['total_amount'].sum().reset_index()\n",
    "route_revenue_lost.columns = ['PULocationID', 'DOLocationID', 'revenue_lost']\n",
    "route_analysis = route_analysis.merge(route_revenue_lost, on=['PULocationID', 'DOLocationID'], how='left').fillna(0)\n",
    "\n",
    "# Add zone names\n",
    "route_analysis = route_analysis.merge(\n",
    "    zones[['LocationID', 'Zone']].rename(columns={'LocationID': 'PULocationID', 'Zone': 'pickup_zone'}),\n",
    "    on='PULocationID',\n",
    "    how='left'\n",
    ")\n",
    "route_analysis = route_analysis.merge(\n",
    "    zones[['LocationID', 'Zone']].rename(columns={'LocationID': 'DOLocationID', 'Zone': 'dropoff_zone'}),\n",
    "    on='DOLocationID',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Filter routes with significant volume\n",
    "route_analysis = route_analysis[route_analysis['trip_count'] >= 100]\n",
    "route_analysis = route_analysis.sort_values('revenue_lost', ascending=False)\n",
    "\n",
    "print(\"\\nüìä TOP 30 HIGH-RISK ROUTES:\")\n",
    "print(route_analysis[['pickup_zone', 'dropoff_zone', 'trip_count', 'problem_rate', 'revenue_lost']].head(30).to_string(index=False))\n",
    "\n",
    "# 4. BOROUGH-LEVEL SUMMARY\n",
    "print(\"\\n\\n4Ô∏è‚É£ BOROUGH-LEVEL SUMMARY\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Pickup borough analysis\n",
    "pickup_borough = df.groupby('pickup_borough').agg({\n",
    "    'is_problematic': ['sum', 'mean'],\n",
    "    'total_amount': ['sum', 'count']\n",
    "}).reset_index()\n",
    "\n",
    "pickup_borough.columns = ['Borough', 'problem_count', 'problem_rate', 'total_revenue', 'trip_count']\n",
    "\n",
    "pickup_borough_lost = df[df['is_problematic'] == 1].groupby('pickup_borough')['total_amount'].sum().reset_index()\n",
    "pickup_borough_lost.columns = ['Borough', 'revenue_lost']\n",
    "pickup_borough = pickup_borough.merge(pickup_borough_lost, on='Borough', how='left').fillna(0)\n",
    "\n",
    "pickup_borough = pickup_borough.sort_values('revenue_lost', ascending=False)\n",
    "\n",
    "print(\"\\nüìä PICKUP BOROUGH ANALYSIS:\")\n",
    "print(pickup_borough.to_string(index=False))\n",
    "\n",
    "print(\"\\n‚úÖ Geographic hotspot analysis complete!\")\n",
    "\n",
    "# Store for visualizations\n",
    "geographic_data = {\n",
    "    'pickup_zones': pickup_analysis,\n",
    "    'dropoff_zones': dropoff_analysis,\n",
    "    'routes': route_analysis,\n",
    "    'boroughs': pickup_borough\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c93d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SECTION 6: VENDOR ACCOUNTABILITY ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nüè¢ VENDOR ACCOUNTABILITY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. VENDOR PERFORMANCE METRICS\n",
    "print(\"\\n1Ô∏è‚É£ CALCULATING VENDOR PERFORMANCE METRICS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "vendor_analysis = df.groupby('VendorID').agg({\n",
    "    'is_dispute': ['sum', 'mean'],\n",
    "    'is_no_charge': ['sum', 'mean'],\n",
    "    'is_voided': ['sum', 'mean'],\n",
    "    'is_problematic': ['sum', 'mean'],\n",
    "    'total_amount': ['sum', 'mean', 'count'],\n",
    "    'fare_per_mile': 'mean',\n",
    "    'tip_amount': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "vendor_analysis.columns = ['VendorID', 'dispute_count', 'dispute_rate',\n",
    "                           'no_charge_count', 'no_charge_rate',\n",
    "                           'voided_count', 'voided_rate',\n",
    "                           'problem_count', 'problem_rate',\n",
    "                           'total_revenue', 'avg_fare', 'trip_count',\n",
    "                           'avg_fare_per_mile', 'avg_tip']\n",
    "\n",
    "# Calculate revenue lost per vendor\n",
    "vendor_revenue_lost = df[df['is_problematic'] == 1].groupby('VendorID')['total_amount'].sum().reset_index()\n",
    "vendor_revenue_lost.columns = ['VendorID', 'revenue_lost']\n",
    "vendor_analysis = vendor_analysis.merge(vendor_revenue_lost, on='VendorID', how='left').fillna(0)\n",
    "\n",
    "# Calculate store-and-forward rate\n",
    "vendor_saf = df[df['store_and_fwd_flag'] == 'Y'].groupby('VendorID').size().reset_index(name='saf_count')\n",
    "vendor_analysis = vendor_analysis.merge(vendor_saf, on='VendorID', how='left').fillna(0)\n",
    "vendor_analysis['saf_rate'] = vendor_analysis['saf_count'] / vendor_analysis['trip_count']\n",
    "\n",
    "# Vendor name mapping (if known)\n",
    "vendor_names = {\n",
    "    1: 'Creative Mobile Technologies',\n",
    "    2: 'VeriFone Inc.'\n",
    "}\n",
    "vendor_analysis['VendorName'] = vendor_analysis['VendorID'].map(vendor_names).fillna('Unknown')\n",
    "\n",
    "print(\"\\nüìä VENDOR PERFORMANCE COMPARISON:\")\n",
    "print(vendor_analysis[['VendorName', 'trip_count', 'problem_rate', 'dispute_rate', \n",
    "                       'avg_fare', 'revenue_lost', 'saf_rate']].to_string(index=False))\n",
    "\n",
    "# 2. STATISTICAL TESTING\n",
    "print(\"\\n\\n2Ô∏è‚É£ STATISTICAL SIGNIFICANCE TESTING\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if len(vendor_analysis) > 1:\n",
    "    # Chi-square test for dispute rates\n",
    "    from scipy.stats import chi2_contingency\n",
    "    \n",
    "    contingency_table = pd.crosstab(df['VendorID'], df['is_problematic'])\n",
    "    chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "    \n",
    "    print(f\"\\nüìä Chi-Square Test for Problem Rate Differences:\")\n",
    "    print(f\"   Chi-square statistic: {chi2:.4f}\")\n",
    "    print(f\"   P-value: {p_value:.6f}\")\n",
    "    print(f\"   Degrees of freedom: {dof}\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(f\"   ‚úÖ SIGNIFICANT: Vendor problem rates differ significantly (p < 0.05)\")\n",
    "    else:\n",
    "        print(f\"   ‚Üí NOT SIGNIFICANT: Vendor problem rates are similar (p >= 0.05)\")\n",
    "    \n",
    "    # Calculate relative risk\n",
    "    overall_problem_rate = df['is_problematic'].mean()\n",
    "    \n",
    "    print(f\"\\nüìä RELATIVE RISK ANALYSIS:\")\n",
    "    print(f\"   Overall problem rate: {overall_problem_rate*100:.3f}%\")\n",
    "    \n",
    "    for idx, row in vendor_analysis.iterrows():\n",
    "        relative_risk = (row['problem_rate'] / overall_problem_rate - 1) * 100\n",
    "        print(f\"   {row['VendorName']}: {relative_risk:+.1f}% vs average\")\n",
    "\n",
    "# 3. VENDOR TECHNOLOGY ISSUES\n",
    "print(\"\\n\\n3Ô∏è‚É£ TECHNOLOGY & CONNECTIVITY ANALYSIS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Analyze correlation between store-and-forward and problems\n",
    "saf_problem_corr = df[['store_and_fwd_flag', 'is_problematic']].copy()\n",
    "saf_problem_corr['saf_numeric'] = (saf_problem_corr['store_and_fwd_flag'] == 'Y').astype(int)\n",
    "correlation = saf_problem_corr['saf_numeric'].corr(saf_problem_corr['is_problematic'])\n",
    "\n",
    "print(f\"\\nüìä Store-and-Forward vs Problem Correlation: {correlation:.4f}\")\n",
    "\n",
    "saf_trips = df[df['store_and_fwd_flag'] == 'Y']\n",
    "normal_trips = df[df['store_and_fwd_flag'] == 'N']\n",
    "\n",
    "saf_problem_rate = saf_trips['is_problematic'].mean() if len(saf_trips) > 0 else 0\n",
    "normal_problem_rate = normal_trips['is_problematic'].mean() if len(normal_trips) > 0 else 0\n",
    "\n",
    "print(f\"\\nüìä Problem Rates by Connection Status:\")\n",
    "print(f\"   Store-and-forward trips: {saf_problem_rate*100:.3f}%\")\n",
    "print(f\"   Normal trips: {normal_problem_rate*100:.3f}%\")\n",
    "print(f\"   Difference: {(saf_problem_rate - normal_problem_rate)*100:+.3f} percentage points\")\n",
    "\n",
    "if saf_problem_rate > normal_problem_rate * 1.2:\n",
    "    print(f\"   ‚ö†Ô∏è ALERT: Connectivity issues linked to {((saf_problem_rate/normal_problem_rate)-1)*100:.1f}% MORE problems\")\n",
    "\n",
    "# 4. PAYMENT TYPE BY VENDOR\n",
    "print(\"\\n\\n4Ô∏è‚É£ PAYMENT TYPE DISTRIBUTION BY VENDOR\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "payment_by_vendor = pd.crosstab(df['VendorID'], df['payment_type_label'], normalize='index') * 100\n",
    "payment_by_vendor = payment_by_vendor.merge(\n",
    "    vendor_analysis[['VendorID', 'VendorName']],\n",
    "    left_index=True,\n",
    "    right_on='VendorID'\n",
    ").set_index('VendorName')\n",
    "\n",
    "print(\"\\nüìä Payment Type Distribution (%):\")\n",
    "print(payment_by_vendor.to_string())\n",
    "\n",
    "print(\"\\n‚úÖ Vendor accountability analysis complete!\")\n",
    "\n",
    "# Store for visualizations\n",
    "vendor_data = {\n",
    "    'performance': vendor_analysis,\n",
    "    'payment_distribution': payment_by_vendor\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fe4f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SECTION 7: ROOT CAUSE ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nüîç ROOT CAUSE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ COMPARING PROBLEM TRIPS vs NORMAL TRIPS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Separate trip types\n",
    "normal_trips = df[df['payment_type'].isin([1, 2])]  # Credit card or Cash\n",
    "dispute_trips = df[df['is_dispute'] == 1]\n",
    "no_charge_trips = df[df['is_no_charge'] == 1]\n",
    "voided_trips = df[df['is_voided'] == 1]\n",
    "\n",
    "# Define metrics to compare\n",
    "metrics = [\n",
    "    'trip_distance',\n",
    "    'trip_duration_minutes',\n",
    "    'fare_amount',\n",
    "    'fare_per_mile',\n",
    "    'fare_per_minute',\n",
    "    'speed_mph',\n",
    "    'tip_amount',\n",
    "    'passenger_count',\n",
    "    'tolls_amount',\n",
    "    'is_airport',\n",
    "    'is_rush_hour',\n",
    "    'is_weekend'\n",
    "]\n",
    "\n",
    "# Calculate averages for each group\n",
    "comparison_data = {\n",
    "    'Metric': [],\n",
    "    'Normal': [],\n",
    "    'Dispute': [],\n",
    "    'No Charge': [],\n",
    "    'Voided': []\n",
    "}\n",
    "\n",
    "for metric in metrics:\n",
    "    if metric in df.columns:\n",
    "        comparison_data['Metric'].append(metric)\n",
    "        comparison_data['Normal'].append(normal_trips[metric].mean())\n",
    "        comparison_data['Dispute'].append(dispute_trips[metric].mean() if len(dispute_trips) > 0 else 0)\n",
    "        comparison_data['No Charge'].append(no_charge_trips[metric].mean() if len(no_charge_trips) > 0 else 0)\n",
    "        comparison_data['Voided'].append(voided_trips[metric].mean() if len(voided_trips) > 0 else 0)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\nüìä TRIP CHARACTERISTICS COMPARISON:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# 2. STATISTICAL TESTING\n",
    "print(\"\\n\\n2Ô∏è‚É£ STATISTICAL SIGNIFICANCE TESTING (T-Tests)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "significant_differences = []\n",
    "\n",
    "for metric in ['trip_distance', 'trip_duration_minutes', 'fare_amount', 'fare_per_mile', 'speed_mph']:\n",
    "    if metric in df.columns:\n",
    "        normal_values = normal_trips[metric].dropna()\n",
    "        dispute_values = dispute_trips[metric].dropna()\n",
    "        \n",
    "        if len(dispute_values) > 30:  # Minimum sample size\n",
    "            t_stat, p_value = ttest_ind(normal_values, dispute_values)\n",
    "            \n",
    "            normal_mean = normal_values.mean()\n",
    "            dispute_mean = dispute_values.mean()\n",
    "            percent_diff = ((dispute_mean - normal_mean) / normal_mean) * 100\n",
    "            \n",
    "            print(f\"\\nüìä {metric}:\")\n",
    "            print(f\"   Normal mean: {normal_mean:.2f}\")\n",
    "            print(f\"   Dispute mean: {dispute_mean:.2f}\")\n",
    "            print(f\"   Difference: {percent_diff:+.1f}%\")\n",
    "            print(f\"   P-value: {p_value:.6f}\")\n",
    "            \n",
    "            if p_value < 0.05:\n",
    "                print(f\"   ‚úÖ SIGNIFICANT difference (p < 0.05)\")\n",
    "                significant_differences.append({\n",
    "                    'metric': metric,\n",
    "                    'percent_diff': percent_diff,\n",
    "                    'p_value': p_value\n",
    "                })\n",
    "            else:\n",
    "                print(f\"   ‚Üí Not significant (p >= 0.05)\")\n",
    "\n",
    "# 3. KEY INSIGHTS\n",
    "print(\"\\n\\n3Ô∏è‚É£ KEY ROOT CAUSE INSIGHTS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if significant_differences:\n",
    "    print(\"\\nüéØ STATISTICALLY SIGNIFICANT DIFFERENCES:\")\n",
    "    for diff in sorted(significant_differences, key=lambda x: abs(x['percent_diff']), reverse=True):\n",
    "        print(f\"   ‚Ä¢ {diff['metric']}: {diff['percent_diff']:+.1f}% difference\")\n",
    "\n",
    "# 4. DISPUTE TRIGGERS ANALYSIS\n",
    "print(\"\\n\\n4Ô∏è‚É£ COMMON DISPUTE TRIGGERS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "dispute_triggers = []\n",
    "\n",
    "# High fare disputes\n",
    "high_fare_disputes = dispute_trips[dispute_trips['fare_amount'] > dispute_trips['fare_amount'].quantile(0.75)]\n",
    "if len(high_fare_disputes) > 0:\n",
    "    pct = (len(high_fare_disputes) / len(dispute_trips)) * 100\n",
    "    dispute_triggers.append(f\"High fares (>{dispute_trips['fare_amount'].quantile(0.75):.0f}): {pct:.1f}% of disputes\")\n",
    "\n",
    "# Long distance disputes\n",
    "long_distance_disputes = dispute_trips[dispute_trips['trip_distance'] > 20]\n",
    "if len(long_distance_disputes) > 0:\n",
    "    pct = (len(long_distance_disputes) / len(dispute_trips)) * 100\n",
    "    dispute_triggers.append(f\"Long trips (>20 miles): {pct:.1f}% of disputes\")\n",
    "\n",
    "# Long duration disputes\n",
    "long_duration_disputes = dispute_trips[dispute_trips['trip_duration_minutes'] > 60]\n",
    "if len(long_duration_disputes) > 0:\n",
    "    pct = (len(long_duration_disputes) / len(dispute_trips)) * 100\n",
    "    dispute_triggers.append(f\"Long duration (>60 min): {pct:.1f}% of disputes\")\n",
    "\n",
    "# Airport disputes\n",
    "airport_disputes = dispute_trips[dispute_trips['is_airport'] == 1]\n",
    "if len(airport_disputes) > 0:\n",
    "    pct = (len(airport_disputes) / len(dispute_trips)) * 100\n",
    "    dispute_triggers.append(f\"Airport trips: {pct:.1f}% of disputes\")\n",
    "\n",
    "# Rush hour disputes\n",
    "rush_disputes = dispute_trips[dispute_trips['is_rush_hour'] == 1]\n",
    "if len(rush_disputes) > 0:\n",
    "    pct = (len(rush_disputes) / len(dispute_trips)) * 100\n",
    "    dispute_triggers.append(f\"Rush hour: {pct:.1f}% of disputes\")\n",
    "\n",
    "print(\"\\nüéØ TOP DISPUTE TRIGGERS:\")\n",
    "for trigger in dispute_triggers:\n",
    "    print(f\"   ‚Ä¢ {trigger}\")\n",
    "\n",
    "print(\"\\n‚úÖ Root cause analysis complete!\")\n",
    "\n",
    "# Store for reporting\n",
    "root_cause_data = {\n",
    "    'comparison': comparison_df,\n",
    "    'significant_differences': significant_differences,\n",
    "    'dispute_triggers': dispute_triggers\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28135baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SECTION 8: FRAUD DETECTION SCORING\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nüö® FRAUD DETECTION SCORING SYSTEM\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. BUILD RULE-BASED SCORING FUNCTION\n",
    "print(\"\\n1Ô∏è‚É£ BUILDING RULE-BASED FRAUD RISK SCORING\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "def calculate_fraud_score(row):\n",
    "    \"\"\"\n",
    "    Calculate fraud risk score based on multiple rule-based criteria\n",
    "    Returns: integer score (0-25+)\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    \n",
    "    # Fare per mile anomalies\n",
    "    if pd.notna(row['fare_per_mile']):\n",
    "        if row['fare_per_mile'] < 2 or row['fare_per_mile'] > 20:\n",
    "            score += 3\n",
    "    \n",
    "    # Suspicious distance-fare combinations\n",
    "    if row['trip_distance'] > 50 and row['fare_amount'] < 100:\n",
    "        score += 5\n",
    "    if row['trip_distance'] < 0.5 and row['fare_amount'] > 50:\n",
    "        score += 4\n",
    "    \n",
    "    # Passenger count anomaly\n",
    "    if row['passenger_count'] > 6:\n",
    "        score += 2\n",
    "    \n",
    "    # Tip anomaly (for credit card only)\n",
    "    if row['payment_type'] == 1 and row['tip_amount'] > row['fare_amount']:\n",
    "        score += 4\n",
    "    \n",
    "    # Speed anomaly\n",
    "    if pd.notna(row['speed_mph']):\n",
    "        if row['trip_duration_minutes'] < 2 and row['trip_distance'] > 5:\n",
    "            score += 5\n",
    "        if row['speed_mph'] > 80:\n",
    "            score += 3\n",
    "    \n",
    "    # Problem payment types\n",
    "    if row['payment_type'] in [3, 4, 6]:\n",
    "        score += 3\n",
    "    \n",
    "    # Zero fare but positive total\n",
    "    if row['fare_amount'] == 0 and row['total_amount'] > 0:\n",
    "        score += 4\n",
    "    \n",
    "    # Extremely long trips\n",
    "    if row['trip_duration_minutes'] > 180:\n",
    "        score += 3\n",
    "    \n",
    "    # Extreme short trips with high fare\n",
    "    if row['trip_duration_minutes'] < 1 and row['fare_amount'] > 20:\n",
    "        score += 5\n",
    "    \n",
    "    return score\n",
    "\n",
    "print(\"   ‚úì Fraud scoring function defined\")\n",
    "\n",
    "# 2. CALCULATE FRAUD SCORES\n",
    "print(\"\\n2Ô∏è‚É£ CALCULATING FRAUD RISK SCORES FOR ALL TRIPS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Apply fraud scoring (sample first to test if memory allows full dataset)\n",
    "print(\"   Calculating scores... (this may take a moment)\")\n",
    "\n",
    "# Calculate scores\n",
    "df['fraud_risk_score'] = df.apply(calculate_fraud_score, axis=1)\n",
    "\n",
    "print(f\"   ‚úÖ Fraud scores calculated for {len(df):,} trips\")\n",
    "\n",
    "# 3. CLASSIFY RISK LEVELS\n",
    "print(\"\\n3Ô∏è‚É£ CLASSIFYING TRIPS BY RISK LEVEL\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "def classify_risk(score):\n",
    "    if score <= 2:\n",
    "        return 'Low Risk'\n",
    "    elif score <= 5:\n",
    "        return 'Medium Risk'\n",
    "    elif score <= 9:\n",
    "        return 'High Risk'\n",
    "    else:\n",
    "        return 'Critical Risk'\n",
    "\n",
    "df['risk_category'] = df['fraud_risk_score'].apply(classify_risk)\n",
    "\n",
    "# Risk distribution\n",
    "risk_distribution = df['risk_category'].value_counts().sort_index()\n",
    "print(\"\\nüìä FRAUD RISK DISTRIBUTION:\")\n",
    "for category, count in risk_distribution.items():\n",
    "    pct = (count / len(df)) * 100\n",
    "    print(f\"   {category}: {count:,} trips ({pct:.2f}%)\")\n",
    "\n",
    "# 4. FINANCIAL IMPACT BY RISK CATEGORY\n",
    "print(\"\\n\\n4Ô∏è‚É£ FINANCIAL IMPACT BY RISK CATEGORY\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "risk_financial = df.groupby('risk_category').agg({\n",
    "    'total_amount': ['sum', 'mean', 'count'],\n",
    "    'is_problematic': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "risk_financial.columns = ['Risk Category', 'Total Revenue', 'Avg Fare', 'Trip Count', 'Problem Rate']\n",
    "\n",
    "print(\"\\nüìä REVENUE AT RISK:\")\n",
    "print(risk_financial.to_string(index=False))\n",
    "\n",
    "# Calculate high-risk revenue\n",
    "high_risk_revenue = df[df['risk_category'].isin(['High Risk', 'Critical Risk'])]['total_amount'].sum()\n",
    "total_revenue = df['total_amount'].sum()\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è HIGH & CRITICAL RISK TRIPS:\")\n",
    "print(f\"   Total revenue: ${high_risk_revenue:,.2f}\")\n",
    "print(f\"   Percentage of total: {(high_risk_revenue/total_revenue)*100:.2f}%\")\n",
    "\n",
    "# 5. TOP HIGH-RISK EXAMPLES\n",
    "print(\"\\n\\n5Ô∏è‚É£ EXAMPLES OF HIGH-RISK TRIPS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "high_risk_examples = df[df['risk_category'] == 'Critical Risk'].nlargest(10, 'fraud_risk_score')\n",
    "\n",
    "print(\"\\nüìä TOP 10 CRITICAL RISK TRIPS:\")\n",
    "display_cols = ['fraud_risk_score', 'trip_distance', 'trip_duration_minutes', \n",
    "                'fare_amount', 'total_amount', 'payment_type_label', \n",
    "                'fare_per_mile', 'speed_mph']\n",
    "print(high_risk_examples[display_cols].to_string(index=False))\n",
    "\n",
    "# 6. SCORE DISTRIBUTION STATISTICS\n",
    "print(\"\\n\\n6Ô∏è‚É£ FRAUD SCORE STATISTICS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\nüìä Score Distribution:\")\n",
    "print(f\"   Mean: {df['fraud_risk_score'].mean():.2f}\")\n",
    "print(f\"   Median: {df['fraud_risk_score'].median():.0f}\")\n",
    "print(f\"   Std Dev: {df['fraud_risk_score'].std():.2f}\")\n",
    "print(f\"   Max: {df['fraud_risk_score'].max():.0f}\")\n",
    "print(f\"   75th percentile: {df['fraud_risk_score'].quantile(0.75):.0f}\")\n",
    "print(f\"   95th percentile: {df['fraud_risk_score'].quantile(0.95):.0f}\")\n",
    "print(f\"   99th percentile: {df['fraud_risk_score'].quantile(0.99):.0f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Fraud detection scoring complete!\")\n",
    "\n",
    "# Store for reporting\n",
    "fraud_data = {\n",
    "    'risk_distribution': risk_distribution,\n",
    "    'risk_financial': risk_financial,\n",
    "    'high_risk_examples': high_risk_examples\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d983d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd350914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick diagnostic cell\n",
    "try:\n",
    "    print(f\"‚úÖ df exists with {len(df):,} records\")\n",
    "    print(f\"‚úÖ Columns available: {len(df.columns)}\")\n",
    "except NameError:\n",
    "    print(\"‚ùå df not defined - run cells 1-8 first\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
